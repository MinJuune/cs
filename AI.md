# 1. Overfitting이 무엇인지, 그리고 Overfitting을 방지하기 위한 방법으로 무엇이 있는지 설명해 보세요.  

**Overfitting**은 학습 데이터에는 잘 맞지만, 새로운 데이터에서는 성능이 떨어지는 현상을 의미합니다.  
즉, 모델이 데이터의 패턴뿐만 아니라, 노이즈까지 학습하여, 일반화 능력이 떨어진 상태입니다.  
Overfitting을 방지하기 위한 방법으로는 데이터 증강, 더 많은 데이터 확보, Dropout, Batch Normalization, L1/L2 Regularization, Early Stopping, Cross Validation 등이 있습니다.  

<br><br>

# 2. Dropout, Batch Normalization, L1/L2 Regularization, Cross Validation에 대해 설명해 보세요.  

**Dropout**은 학습 중 일부 뉴런을 임의로 비활성화하여, 특정 노드에 과도하게 의존하지 않도록 하는 방법입니다.  
**Batch Normalization**은 활성화 함수에 입력되기 전의 선형 변환을 정규화하여, 학습을 안정화시키는 방법입니다.  
구체적으로는 입력 데이터를 평균 0, 분산 1로 정규화한 뒤, 학습 가능한 파라미터를 적용해 모델이 필요한 스케일과 이동을 학습할 수 있도록 합니다.  
**Regularization**은 모델의 복잡도를 제어하여 Overfitting을 방지하는 기법입니다. 
L1 Regularization은 가중치의 절대값 합에 패널티를 주어, 필요 없는 가중치를 줄이는 방식으로, 모델의 복잡도를 줄이는 방식이고,  
L2 Regularization은 가중치의 제곱합에 패널티를 주어, 가중치가 과도하게 커지는 것을 방지하는 방식으로, 모델의 복잡도를 줄이는 방식입니다.  
**Cross Validation**은 데이터를 여러 부분으로 나누고, 각 부분을 번갈아가며 validation set으로 사용하는 방식으로, 데이터 양이 적을 때 사용할 수 있는 방식입니다.  

<br><br>  

# 3. 활성화 함수에 대해 설명해 보세요.  

**활성화 함수**는 신경망에 비선형성을 부여하여, 더 큰 표현력을 갖게 하고, 복잡한 문제를 해결할 수 있게 해주는 역할을 합니다.  
만약 활성화 함수가 없다면, 여러 층을 쌓더라도, 여러 선형 변환의 조합이 결국 하나의 선형 변환과 동일한 표현력을 갖게 됩니다.  
대표적인 활성화 함수로 Sigmoid, ReLU 등이 있습니다.  

<br><br>  

# 4. 손실 함수에 대해 설명해 보세요.  

**손실 함수**는 모델의 예측 값과 실제 값의 차이를 수치로 표현하는 함수입니다.  
학습 과정에서는 이 손실 함수를 최도화하도록 파라미터를 업데이트하며,  
모델이 얼마나 잘 학습되고 있는지 평가하는 기준이 됩니다.  
주요 손실 함수로는 MSE, Cross-Entropy 등이 있습니다.  

<br><br>  

# 5. 역전파 알고리즘과 경사 하강법에 대해 설명해 보세요.  

**역전파**는 손실 함수를 각 파라미터에 대해 미분한 기울기 값을 구하는 과정이고,  
**경사 하강법**은 그 기울기를 사용해, 손실 함수가 최소가 되도록, 파라미터를 업데이트하는, 최적화 과정입니다.  
즉, 역전파는 gradient를 구하는 과정, 경사 하강법은 그 gradient를 사용해 학습하는 방법입니다.  

<br><br>  

# 6. 경사 하강법의 변형 방법 (예: SGD, Adam)에 대해 설명해 보세요.  

경사 하강법은 전체 데이터의 기울기를 사용해 계산하는데, 연산량을 줄이고 학습 속도를 높이기 위한 여러 최적화 방법이 있습니다.  
SGD는 전체 데이터가 아니라, 일부 미니배치 데이터의 기울기를 사용해, 연산량을 줄이고 학습 속도를 높이는 방법입니다.  
Adam은 이전 기울기의 평균과 분산 등 정보를 함께 고려해, 각 파라미터별로 학습률을 자동 조정하는 방법입니다.  
이로 인해 학습이 안정적이고, 학습 속도가 빠르다는 장점이 있습니다.  

<br><br>  

# 7. Gradient Vanishing과 Gradient Exploding에 대해 설명해 보세요.  

**Gradient Vanishing**은 네트워크의 깊이가 깊어질수록, 기울기가 작아져 학습이 어려워지는 문제입니다.  
역전파 과정에서, chain rule에 의해 기울기가 연속적으로 곱해지게 됩니다.  
그런데, 활성화 함수의 미분값이 1보다 작은 경우, 기울기가 연속적으로 곱해지게 되면, 초기층으로 전달되는 기울기가 거의 0에 가까워지게 됩니다.  
그러면 파라미터가 업데이트되지 않고, 학습이 멈추는 현상이 발생합니다.  
**Gradient Exploding**은 가중치가 너무 커져서, 학습이 불안정해지는 문제입니다.  

<br><br>  

# 8. 편향(Bias)과 분산(Variance)의 개념을 설명하고, 이 두가지가 모델의 성능에 미치는 영향에 대해 설명해 보세요.  

**Bias**는 모델이 훈련 데이터의 실제 관계를 제대로 학습하지 못해 생기는 오류를 의미하며, 단순한 모델일 수록 Bias가 높아져 underfitting이 발생합니다.  
**Variance**는 모델이 훈련 데이터의 작은 변화에도 민감하게 반응하는 정도를 의미하며, 복잡한 모델일 수록 Variance가 높아져 overfitting이 발생합니다.  
따라서 모델의 일반화 성능을 높이기 위해서는 Bias와 Variance의 trade off를 고려해야 합니다.  

<br><br>  

# 9. 정규화(Normalization)와 표준화(Standardization)의 차이점에 대해 설명해 보세요.  

**정규화**는 데이터를 특정 범위로 변환하는 과정이며,  
**표준화**는 데이터를 평균 0, 표준 편차 1로 변화하는 과정입니다.  
두 방법 모두 데이터의 스케일을 조정하여 분석의 일관성을 높입니다.  

<br><br>  

# 10. SVM의 기본 원리와 커널 트릭의 역할을 설명해 보세요.  

**SVM**은 데이터를 선형적으로 분리하는 최적의 결정 경계를 찾는 분류 알고리즘입니다.  
주요 아이디어로는 데이터 포인트와 결정 경계간의 최대 마진을 찾는 것입니다.  
만약 데이터가 비선형적으로 분포되었을 경우, 커널 트릭을 이용해 고차원으로 매핑하여 선형적으로 분리 가능하게 만들 수 있습니다.  

<br><br>  

# 11. PCA에 대해 설명해 보세요.  

**PCA**는 고차원 데이터를 저차원으로 축소하면서도, 정보 손실을 최소화하기 위한 차원 축소 기법입니다.  
데이터의 분산이 가장 큰 방향을 찾아 주성분으로 정하고, 고유값 분해를 통해, 분산이 큰 순서대로 축을 선택합니다.  
이렇게 선택된 주성분 축에 데이터를 projection하면, 고차원 데이터를 저차원으로 축소하면서도, 정보 손실을 최소화할 수 있습니다.  

<br><br>  

# 12. Learning Rate Schedule에 대해 설명해 보세요.  

**Learning Rate Schedule**은 학습이 진행됨에 따라 Learning Rate를 조정하는 방법입니다.  
학습 초기에는 Learning Rate를 크게 설정해 빠르게 방향을 잡고,  
이후에는 Learning Rate를 줄여 최적점 근처에서 안정적으로 수렴하도록 합니다.  
